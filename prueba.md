# Mentor√≠a - Flujo ETL Marklines

## 1. Origen del archivo Excel
El usuario (cliente) coloca un archivo Excel en una carpeta compartida. Esta puede estar ubicada en:

- **EFS (Elastic File System)**: Disco duro compartido en la nube de AWS, accesible por varias aplicaciones.
- **S3 (Simple Storage Service)**: Sistema de almacenamiento en la nube de AWS. Archivos ("objetos") se guardan dentro de "buckets" (carpetas virtuales).

## 2. Disparo autom√°tico del proceso (Trigger)
Cuando se sube un archivo nuevo a S3, se activa autom√°ticamente una funci√≥n Lambda. Esto se llama un **trigger**, y permite iniciar un proceso sin intervenci√≥n manual.

## 3. Limpieza y transformaci√≥n del Excel
La funci√≥n Lambda:
- Lee el archivo Excel con `openpyxl`.
- Toma solo columnas A a H.
- Renombra las columnas con nombres t√©cnicos (por ejemplo, ‚ÄúGroup‚Äù se convierte en ‚Äúcustomer_marklines‚Äù).
- Reemplaza valores como `'-'` o `'N/A'` por **0** o los ignora.
- Convierte columnas a tipos de datos correctos (texto, entero, decimal).

Esto se trabaja dentro de un **DataFrame**, que es una tabla en memoria gestionada con la librer√≠a `pandas`.

## 4. Conversi√≥n a formato Parquet
El archivo limpio se guarda en formato **Parquet**, que es eficiente y comprimido, ideal para grandes vol√∫menes de datos.

## 5. Subida a la nube (S3)
El archivo `.parquet` se sube a un bucket S3, organizado por a√±o y mes.

Para esto se usa `boto3`, una librer√≠a que permite interactuar con los servicios de AWS desde Python.

## 6. Registro de errores (logging en CloudWatch)
Si algo falla, se guarda un mensaje en **CloudWatch**, un servicio que registra todo lo que hace la funci√≥n. Esto permite revisar fallas despu√©s.

## 7. Visualizaci√≥n en Power BI
Una vez cargados en la base de datos Redshift, los datos son consultados desde Power BI para hacer reportes. Esta parte fue mi responsabilidad.

---

## üîç Temas clave que podr√≠an preguntarte

### ¬øQu√© pasa si el archivo Excel no tiene un identificador √∫nico?
El flujo actual **no valida unicidad de registros**. Cada vez que se sube un Excel, todos los datos se procesan como nuevos. Si hay duplicados, van a pasar tal cual al DataLake y despu√©s a Redshift. Se podr√≠a mejorar a√±adiendo validaciones con un campo ID o combinaciones de columnas √∫nicas.

### ¬øC√≥mo funciona la conexi√≥n a la carpeta compartida?
La funci√≥n Lambda accede a los archivos cuando se suben a un **bucket S3**, que es como una carpeta en la nube. No se conecta directamente a EFS en este ejemplo, aunque el flujo conceptual lo muestra como opci√≥n. Todo se maneja por medio de AWS S3 y `boto3`.

### ¬øQu√© hace exactamente el m√©todo de limpieza de datos?
- Lee columnas espec√≠ficas del Excel.
- Reemplaza valores vac√≠os o no v√°lidos (`'-'`, `'N/A'`) por cero.
- Convierte tipos de datos (por ejemplo, texto a n√∫mero).
- Formatea una fecha que viene como `202401` a formato `01/01/2024`.

### ¬øPor qu√© se usa Parquet y no Excel o CSV?
Parquet es un formato **m√°s r√°pido, liviano y eficiente para an√°lisis de datos**, especialmente en la nube y Big Data. Redshift y otras herramientas de AWS lo leen m√°s f√°cilmente. Ahorramos espacio y tiempo de procesamiento.

### ¬øY si el archivo tiene columnas nuevas o cambia el formato?
Actualmente no hay validaci√≥n de estructura del archivo. El c√≥digo asume que vienen las columnas A‚ÄìH siempre. Si cambia, puede fallar o procesar mal. Una mejora ser√≠a validar la estructura antes de seguir el flujo.

---

## üßë‚Äçüè´ ¬øQu√© podr√≠as decir en la sesi√≥n?

> ‚ÄúMi rol en este flujo fue desarrollar el reporte en Power BI. Para eso, me conect√© directamente a la base de datos Redshift, donde ya llegan los datos limpios desde el proceso de ETL.  
>
> El flujo completo parte de un archivo Excel que el cliente coloca en una carpeta. Ese archivo se sube a un bucket S3, lo que activa autom√°ticamente una funci√≥n Lambda. Esa funci√≥n se encarga de limpiar los datos, convertirlos a formato Parquet, y subirlos nuevamente a S3 organizados por fecha.  
>
> Lo que hace la limpieza es muy espec√≠fico: toma ciertas columnas, corrige valores como '-' o 'N/A', y asegura que todos los datos est√©n en el formato correcto. No hay validaciones de duplicados ni identificadores √∫nicos por ahora.  
>
> Una vez que los datos est√°n en el DataLake, un proceso ETL los mueve a Redshift, y desde ah√≠, se visualizan en Power BI.  
>
> Si se requiere extender o robustecer el flujo, se podr√≠an agregar validaciones de estructura, identificadores √∫nicos y manejo de cambios en el formato del Excel.‚Äù
